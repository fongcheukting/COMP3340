# Lecture 9: 3D Pose Estimation

## 1. Introdution

In this section we will introduce the Human Pose Estimation problem, which is the task of localizing human body keypoints (also known as body parts or joints). Human pose estimation provides detailed geometric and motion information of the human body, which serves as the basis for various downstream tasks, e.g. person re-identification and action recognition.

![](https://i.imgur.com/5EvXEyy.png)
(Figure is from the official webset of COCO dataset)



---

## 2. Related Tasks and High-Level Ideas

### 2.1 2D Human Pose Estimation

2D human pose estimation estimates the 2D location (x-y coordinate) of each human body keypoint from images or videos. The task of 2D human pose estimation can be categorized into single-person and multi-person scenarios.

#### 2.1.1 Single-person Pose Estimation

![](https://i.imgur.com/4Xl4nwp.png)
(Figure from "Deep Learning-Based Human Pose Estimation: A Survey" [[paper link]](https://arxiv.org/abs/2012.13392).)

Single-person pose estimation assumes that the input image only contains one person centered at the image. **Regression-based approaches** directly learn a mapping from the input image to the locations of all body joints. They assume global object features has enough information for accurate localization. However, such approaches often have low accuracy in high precision region. Recent works mainly adopt the heatmap-based approaches. **Heatmap-based approaches** represent each keypoint using a heatmap. The target heatmap is generated by a 2D Gaussian centered at the ground-truth keypoint location. Heatmaps preserve the spatial information which provides strong supervision signals and facilitates the training of convolutional neural networks. 


#### 2.1.2 Multi-person Pose Estimation

![](https://i.imgur.com/mgMzDnn.png)

Multi-person pose estimation does not make any assumption about the number of people and the location of the people in the image. It is much more challenging thant single-person pose estimation. Most multi-person pose estimation approaches can be categorized into top-down and bottom-up approaches. **Top-down approaches** first apply an off-the-shelf object detector to detect human bounding boxes, then estimate keypoints for each detected person. **Bottom-up approaches** directly detect all the keypoints and then group them into people. The processing time of top-down approaches linearly grows with the number of people because they need to process each person separately. In comparison, the inference speed of bottom-up approaches is generally much faster, because they decouple the running time complexity from the number of people in the image.

### 2.2 3D Human Pose Estimation

#### 2.2.1 Single-view 3D Human Pose Estimation
![](https://i.imgur.com/XpXq1O0.png)
(Figure from "Deep Learning-Based Human Pose Estimation: A Survey" [[paper link]](https://arxiv.org/abs/2012.13392).)

3D human pose estimation from a single view of monocular images is nontrivial, due to occlusions, depth ambiguities and insufficient training data. It is an ill-posed problem, because different 3D human poses can be projected to a similar 2D pose. 

Single-view 3D human pose estimation can be categorized into model-free and model-based methods. The difference is whether the human body model is employed to reconstruct 3D human representation. Model-free methods can be further divided into two classes: (a) Direct estimation approaches and (b) 2D-to-3D lifting approaches. 

**Direct estimation approaches** learn to infer 3D human pose directly from 2D images without using intermediate 2D pose representation. 
**2D-to-3D lifting approaches** first employ off-the-shelf 2D human pose estimators to estimate 2D poses, and then lift the 2D pose into 3D space. Benefiting from the excellent performance of recent 2D human pose estimation, such methods generally outperform the direct estimation approaches.


**Model-based approaches** incorporate parametric human body models (e.g. SMPL model) to estimate human pose and shape. SMPL is one of the most popular parametric human body models, which provide detailed pose and shape information. SMPL-based approaches estimate 3D pose and shape parameters from images, and then using a model regressor to output 3D human mesh.


#### 2.2.2 Multi-view 3D Human Pose Estimation

To overcome the problem of occlusion in single-view setting, many works have explore estimating a 3D human pose from multi-view images. To reconstruct 3D pose, multi-view geometry should be considered to solve the association of corresponding location between multiple cameras. 

![](https://i.imgur.com/pZ4FqXx.png)

Most multi-view 3D human pose estimation methods first employ an off-the-shelf 2D human pose estimators to estimate 2D poses for each camera view, and then adopt multi-view 3D reconstruction to obtain 3D poses. 



---

## 3. Representative Methods


### 3.1 Stacked Hourglass
> Stacked Hourglass Networks for Human Pose Estimation. Alejandro Newell, Kaiyu Yang, and Jia Deng. In ECCV, 2016. 

Stacked Hourglass is a popular model for 2D single-person pose estimation. It is a type of convolutional encoder-decoder network. It extracts feature maps from input images and then reconstruct high-resolution heatmap output.

![](https://i.imgur.com/4tOormR.png)

Note that in a deep neural networks, features from different layers contain different levels of information. Low-level features contain spatial information for precise localization and is essential for part detection, but local evidence is weak and lacks understanding of the body parts (what it is). Global features (or context) is strong cue to understand the relationships between body parts and limb occlusion, but it lackes spatial information and the knowledge of the part’s locations in the image (where it is)). Stacked Hourglass network designs an architecture to combine both the low-level and high-level features. 

![](https://i.imgur.com/7lj3AQU.png)


#### Network Architecture

![](https://i.imgur.com/AcDrB0t.png)
A single “hourglass” module (excluding the final 1×1 layers). Each box corresponds to a residual module as below.

![](https://i.imgur.com/UXkrucT.png)
A residual module.


Convolutional and max pooling layers are used to process features down to a very low resolution. After reaching the lowest resolution, the network begins the sequence of upsampling and combination of features across scales. For the upsampling path, the nearest neighbor upsampling of the lower resolution is done followed by an element-wise addition of the two sets of features. After reaching the output resolution of the network, two consecutive rounds of 1×1 convolutions are applied to produce the final network predictions. The final design makes extensive use of residual modules. 



**Sample code.** Hourglass Module
```python
class HourglassModule(nn.Module):
    """Hourglass Module for HourglassNet backbone.
    Generate module recursively and use BasicBlock as the base unit.
    Args:
        depth (int): Depth of current HourglassModule.
        stage_channels (list[int]): Feature channels of sub-modules in current
            and follow-up HourglassModule.
        stage_blocks (list[int]): Number of sub-modules stacked in current and
            follow-up HourglassModule.
        norm_cfg (dict): Dictionary to construct and config norm layer.
    """

    def __init__(self,
                 depth,
                 stage_channels,
                 stage_blocks,
                 norm_cfg=dict(type='BN', requires_grad=True)):
        # Protect mutable default arguments
        norm_cfg = copy.deepcopy(norm_cfg)
        super().__init__()

        self.depth = depth

        cur_block = stage_blocks[0]
        next_block = stage_blocks[1]

        cur_channel = stage_channels[0]
        next_channel = stage_channels[1]

        self.up1 = ResLayer(
            BasicBlock, cur_block, cur_channel, cur_channel, norm_cfg=norm_cfg)

        self.low1 = ResLayer(
            BasicBlock,
            cur_block,
            cur_channel,
            next_channel,
            stride=2,
            norm_cfg=norm_cfg)

        if self.depth > 1:
            self.low2 = HourglassModule(depth - 1, stage_channels[1:],
                                        stage_blocks[1:])
        else:
            self.low2 = ResLayer(
                BasicBlock,
                next_block,
                next_channel,
                next_channel,
                norm_cfg=norm_cfg)

        self.low3 = ResLayer(
            BasicBlock,
            cur_block,
            next_channel,
            cur_channel,
            norm_cfg=norm_cfg,
            downsample_first=False)

        self.up2 = nn.Upsample(scale_factor=2)

    def forward(self, x):
        """Model forward function."""
        up1 = self.up1(x)
        low1 = self.low1(x)
        low2 = self.low2(low1)
        low3 = self.low3(low2)
        up2 = self.up2(low3)
        return up1 + up2
```

#### Intermediate Supervision

![](https://i.imgur.com/Cw4z2CC.png)
The network splits and produces a set of heatmaps (outlined in blue) where a loss can be applied. A 1×1 convolution remaps the heatmaps to match the number of channels of the intermediate features. These are added together along with the features from the preceding hourglass. The same hourglass modules are repeatedly stacked to construct the final network.



---

### 3.2 OpenPose
> OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields. Cao, Zhe, et al. IEEE TPAMI 43.1 (2019): 172-186.

OpenPose is a well-known bottom-up approach for 2D multi-person pose estimation.

![](https://i.imgur.com/uFZqe52.png)
This is the overall pipeline of OpenPose. First, the image is passed through a backbone network (VGG) to extract feature maps. Then, the feature maps are processed with multi-stage CNN to generate: 1) a set of Part Confidence Maps (Heatmaps) and 2) a set of Part Affinity Fields (PAFs). Finally, the Confidence Maps and Part Affinity Fields are processed by a greedy parsing algorithm to assemble the predicted keypoints into 2D poses.

#### Part Confidence Maps (Heatmaps)
![](https://i.imgur.com/NPBQgBh.png)

Part Confidence Map (also known as Heatmap) is a 2D representation of the belief that a particular body part can be located in any given pixel. For multiple people, there should be a peak corresponding to each visible part $j$ for each person $k$.

We first generate individual confidence maps $S^*_{j,k}$ for each person $k$. Let $\mathrm{x}_{j,k} \in \mathbb{R}^2$ be the ground-truth position of body part $j$ of person $k$. The value at location $\mathrm{p} \in \mathbb{R}^2$ is defined as:

\begin{eqnarray}
S^*_{j,k}(\mathrm{p}) = \exp(-\frac{||\mathrm{p}-\mathrm{x}_{j,k}||_2^2}{\sigma^2})
\end{eqnarray}

$\sigma$ denotes the standard deviation. The overall ground-truth confidence maps are:

\begin{eqnarray}
S^*_j(\mathrm{p}) = \max_k S^*_{j,k}(\mathrm{p}).
\end{eqnarray}


**Sample code**. Heatmap Generator
```python
class HeatmapGenerator:
    """Generate heatmaps for bottom-up models.
    Args:
        num_joints (int): Number of keypoints.
        output_size (int): Size of feature map.
        sigma (int): Sigma of the heatmaps.
    """

    def __init__(self, output_size, num_joints, sigma=-1):
        self.output_size = output_size
        self.num_joints = num_joints
        if sigma < 0:
            sigma = self.output_size / 64
        self.sigma = sigma
        size = 6 * sigma + 3

        x = np.arange(0, size, 1, np.float32)
        y = x[:, None]
        x0, y0 = 3 * sigma + 1, 3 * sigma + 1
        self.g = np.exp(-((x - x0)**2 + (y - y0)**2) / (2 * sigma**2))

    def __call__(self, joints):
        """Generate heatmaps."""
        hms = np.zeros((self.num_joints, self.output_size, self.output_size), dtype=np.float32)

        sigma = self.sigma
        for p in joints:
            for idx, pt in enumerate(p):
                # valid keypoints
                if pt[2] > 0:
                    x, y = int(pt[0]), int(pt[1])
                    if x < 0 or y < 0 or x >= self.output_size or y >= self.output_size:
                        continue

                    g = self.g

                    ul = int(np.round(x - 3 * sigma - 1)), int(np.round(y - 3 * sigma - 1))
                    br = int(np.round(x + 3 * sigma + 2)), int(np.round(y + 3 * sigma + 2))

                    c, d = max(0, -ul[0]), min(br[0], self.output_size) - ul[0]
                    a, b = max(0, -ul[1]), min(br[1], self.output_size) - ul[1]

                    cc, dd = max(0, ul[0]), min(br[0], self.output_size)
                    aa, bb = max(0, ul[1]), min(br[1], self.output_size)
                    hms[idx, aa:bb, cc:dd] = np.maximum(hms[idx, aa:bb, cc:dd], g[a:b, c:d])
        return hms
```


#### Part Affinity Fields (PAFs)
![](https://i.imgur.com/6b5IYh1.png)

A Part Affinity Field (PAF) is a set of flow fields that encodes unstructured pairwise relationships between body parts. Each pair of body parts has a PAF, i.e elbow and wrist, etc,.

Consider a single limb shown in the figure below. Let $\mathrm{x}_{j_1,k}^p$ and $\mathrm{x}_{j_2,k}^p$ be the ground-truth 2D positions of body parts $j_1$ and $j_2$ from the limb $c$ of person $k$. If a point $\mathrm{p}$ lies on the limb $c$, the value at $L^*_{c,k}(\mathrm{p})$ is a 2D unit vector pointing from the start joint $j_1$ to the end joint $j_2$. Otherwise, the vector is zero-valued.
![](https://i.imgur.com/s1V050g.png)

Ground-truth Part Affinity Fields (PAFs) are:

\begin{eqnarray}
L^*_{c,k}(\mathrm{p}) = \left\{
\begin{aligned}
\mathrm{v} & , & \text{if p on limb c, k}, \\
0 & , & otherwise.
\end{aligned}
\right.
\end{eqnarray}

Here $\mathrm{v}= \frac{(x_{j_2,k} - x_{j_1,k})}{||x_{j_2,k} - x_{j_1,k}||_2}$ is the unit vector in the direction of the limb. The set of points on the limb is defined as those within a distance threshold of the line segment, i.e., those points $\mathrm{p}$ for which

\begin{eqnarray}
0 \leq \mathrm{v} \cdot (\mathrm{p}-x_{j_1,k}) \leq l_{c,k} \text{ and } |\mathrm{v}_\perp \cdot (\mathrm{p}-x_{j_1,k}) \leq \sigma_l| 
\end{eqnarray}

where the limb width $\sigma_l$ is a distance in pixels, the limb length $l_{c,k}=||x_{j_2,k} - x_{j_1,k}||_2$, and $\mathrm{v}_\perp$ is a vector perpendicular to $\mathrm{v}$.

**Sample code**. PAF Generator
```python
class PAFGenerator:
    """Generate part affinity fields.
    Args:
        output_size (int): Size of feature map.
        limb_width (int): Limb width of part affinity fields.
        skeleton (list[list]): connections of joints.
    """

    def __init__(self, output_size, limb_width, skeleton):
        self.output_size = output_size
        self.limb_width = limb_width
        self.skeleton = skeleton

    def _accumulate_paf_map_(self, pafs, src, dst, count):
        """Accumulate part affinity fields between two given joints.
        Args:
            pafs (np.ndarray[2xHxW]): paf maps (2 dimensions:x axis and
                y axis) for a certain limb connection. This argument will
                be modified inplace.
            src (np.ndarray[2,]): coordinates of the source joint.
            dst (np.ndarray[2,]): coordinates of the destination joint.
            count (np.ndarray[HxW]): count map that preserves the number of non-zero vectors at each point. This argument will be modified inplace.
        """
        limb_vec = dst - src
        norm = np.linalg.norm(limb_vec)
        if norm == 0:
            unit_limb_vec = np.zeros(2)
        else:
            unit_limb_vec = limb_vec / norm

        min_x = max(np.floor(min(src[0], dst[0]) - self.limb_width), 0)
        max_x = min(
            np.ceil(max(src[0], dst[0]) + self.limb_width),
            self.output_size - 1)
        min_y = max(np.floor(min(src[1], dst[1]) - self.limb_width), 0)
        max_y = min(
            np.ceil(max(src[1], dst[1]) + self.limb_width),
            self.output_size - 1)

        range_x = list(range(int(min_x), int(max_x + 1), 1))
        range_y = list(range(int(min_y), int(max_y + 1), 1))

        mask = np.zeros_like(count, dtype=bool)
        if len(range_x) > 0 and len(range_y) > 0:
            xx, yy = np.meshgrid(range_x, range_y)
            delta_x = xx - src[0]
            delta_y = yy - src[1]
            dist = np.abs(delta_x * unit_limb_vec[1] -
                          delta_y * unit_limb_vec[0])
            mask_local = (dist < self.limb_width)
            mask[yy, xx] = mask_local

        pafs[0, mask] += unit_limb_vec[0]
        pafs[1, mask] += unit_limb_vec[1]
        count += mask

        return pafs, count

    def __call__(self, joints):
        """Generate the target part affinity fields."""
        pafs = np.zeros(
            (len(self.skeleton) * 2, self.output_size, self.output_size),
            dtype=np.float32)

        for idx, sk in enumerate(self.skeleton):
            count = np.zeros((self.output_size, self.output_size),
                             dtype=np.float32)

            for p in joints:
                src = p[sk[0]]
                dst = p[sk[1]]
                if src[2] > 0 and dst[2] > 0:
                    self._accumulate_paf_map_(pafs[2 * idx:2 * idx + 2],
                                              src[:2], dst[:2], count)

            pafs[2 * idx:2 * idx + 2] /= np.maximum(count, 1)

        return pafs
```


#### Network Architecture
![](https://i.imgur.com/2hOYDs5.png)



#### Parsing Algorithm

![](https://i.imgur.com/t55QBI1.png)

The problem is formulated as a graph matching problem. The goal is to find a matching with maximum weight for the chosen edges. However, the problem is a NP-Hard problem.

To solve this, we add two relaxations to the optimization problem.

1. Instead of using a complete graph (see b), we use a tree-structure model (see c).
2. Decompose the matching problem into a set of bipartite matching subproblems (see d).



---

### 3.3 SimpleBaseline3D

> A simple yet effective baseline for 3d human pose estimation. Martinez, J., Hossain, R., Romero, J., & Little, J. J. In ICCV , 2017.


SimpleBaseline3D is a 3D single-person model-free human pose estimation approach. It decouples 3d pose estimation as: 

	3D pose = 2d pose + 2d-to-3d lifting

**Advantages**:
1. Make use of off-the-shelf state-of-the-art 2d pose estimation systems
2. Train data-hungry algorithms for the 2d-to-3d problem with large amounts of 3d mocap data captured in controlled environments


#### Error Analysis

Such a model mainly has two sources of errors:
1. 2d pose estimation: A failure to detect 2d poses
2. Pose lifting: a failure to map 2d poses into 3d positions

![](https://i.imgur.com/UO3Bcz1.png)

From the experiments we find that
1. “lifting” ground-truth 2d joint locations to 3d space -> remarkably low error rate (45.5 mm)
2. “lifting” predicted 2d poses from an off-the-shelf 2d detector -> state of the art 3d results (62.9 mm)

**Conclusion**: 2d-to-3d lifting is relatively accurate. And the most errors come from 2d human pose estimation. More attention should be focus on improving the quality of 2d human pose estimation.


#### Network Architecture
![](https://i.imgur.com/8Q9RBud.png)
A simple baseline model for 3d human pose estimation by Martinez et al.

The model architecture is simple. It composes of two fully-connected layers. It also adopts several existing techniques to improve model performance. Linear-RELU layers [1], Residual connections [2], Batch normalization [3], and Dropout [4].

![](https://i.imgur.com/72p2bo8.png)

    [1] V. Nair and G. E. Hinton. Rectified linear units improve restricted Boltzmann machines. In ICML, pages 807–814, 2010.
    [2] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016
    [3] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015
    [4] N. Srivastava et al. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 15(1), 2014.


**Sample code.** Network Architecture
```python
self.conv1 = nn.Sequential(
    ConvModule(
        in_channels,
        mid_channels,
        kernel_size=kernel_size,
        stride=self.stride,
        dilation=self.dilation,
        bias='auto',
        conv_cfg=conv_cfg,
        norm_cfg=norm_cfg))
self.conv2 = nn.Sequential(
    ConvModule(
        mid_channels,
        out_channels,
        kernel_size=1,
        bias='auto',
        conv_cfg=conv_cfg,
        norm_cfg=norm_cfg))

if residual and in_channels != out_channels:
    self.short_cut = build_conv_layer(conv_cfg, in_channels,
                                      out_channels, 1)
else:
    self.short_cut = None

self.dropout = nn.Dropout(dropout) if dropout > 0 else None
```

**Sample code.** Model Forward Function
```python
def forward(self, x):
    """Forward function."""
    if self.use_stride_conv:
        assert self.causal_shift + self.kernel_size // 2 < x.shape[2]
    else:
        assert 0 <= self.pad + self.causal_shift < x.shape[2] - \
            self.pad + self.causal_shift <= x.shape[2]

    out = self.conv1(x)
    if self.dropout is not None:
        out = self.dropout(out)

    out = self.conv2(out)
    if self.dropout is not None:
        out = self.dropout(out)

    if self.residual:
        if self.use_stride_conv:
            res = x[:, :, self.causal_shift +
                    self.kernel_size // 2::self.kernel_size]
        else:
            res = x[:, :,
                    (self.pad + self.causal_shift):(x.shape[2] - self.pad +
                                                    self.causal_shift)]

        if self.short_cut is not None:
            res = self.short_cut(res)
        out = out + res

    return out
```



---

### 3.4 HMR
> End-to-end recovery of human shape and pose. Kanazawa, Angjoo, et al. CVPR 2018

HMR is a popular 3D single-person model-based approach. It directly learns a mapping from image pixels to SMPL parameters using a DNN.

![](https://i.imgur.com/Qizz7Et.png)


#### SMPL introduction
![](https://i.imgur.com/wyVXKYW.png)

SMPL (Skinned Multi-Person Linear) model is a statistical model that encodes the human subjects with two types of parameters:
**Shape parameter**: a shape vector of 10 scalar values, each of which could be interpreted as an amount of expansion/shrink of a human subject along some direction such as taller or shorter.
**Pose parameter**: a pose vector of 24x3 scalar values that keeps the relative rotations of joints with respective to their parameters. Each rotation is encoded as a arbitrary 3D vector in axis-angle rotation representation.

![](https://i.imgur.com/WwwYbVV.png)

HMR model directly predicts these SMPL parameters using a DNN.

#### Pose Discriminator

![](https://i.imgur.com/HiQXCNK.png)

There are large-scale 2D keypoint annotations of in-the-wild images and a separate large-scale dataset of 3D meshes of people with various poses and shapes. The key idea is to take advantage of these unpaired 2D keypoint annotations and 3D scans in a conditional generative adversarial manner. 

Given an image, the network infers the 3D mesh and the camera parameters such that the 3D keypoints match the annotated 2D keypoints after projection. However, single-view pose estimation suffers depth ambiguity. Although the projected 3D poses can well align with the 2D keypoints, the 3D body shape and pose are not realistic.

To deal with ambiguities, these parameters are sent to a discriminator network, whose task is to determine if the 3D parameters correspond to bodies of real humans or not. Hence the network is encouraged to output parameters on the human manifold and the discriminator acts as weak supervision. The network implicitly learns the angle limits for each joint and is discouraged from making people with unusual body shapes.

---

### 3.5 Direct Linear Transform (DLT)

In this section, we first introduce some preliminaries about the camera model. 


#### Pinhole Camera Model

![](https://i.imgur.com/s1XrYdu.png)
Figure from "14 Lectures on Visual SLAM: From Theory to Practice" Xiang Gao et al. 2017.

The process of projecting a 3D point (in meters) to a 2D image plane (in pixels) can be described by a geometric model. For simplicity, we will introduce the pinhole model.

Let $O-x-y-z$ be the camera coordinate system, also known as the camera reference frame. $O$ is the camera's optical center, which is also the ``hole'' in the pinhole model. The 3D point $P$, after being projected through the hole $O$, falls on the physical imaging plane $O'-x'-y'$ and produces the image point $P'$. Let the coordinates of $P$ be $[X,Y,Z]^T$, $P'$ is $[X',Y']^T$, and set the physical distance from the imaging plane to camera plane is $f_x$ and $f_y$ (focal length). Then, according to the similarity of the triangles, there are:

\begin{equation}
𝑋′=𝑓_𝑥 \cdot \frac{𝑋}{𝑍} + 𝑐_𝑥 \\
𝑌′=𝑓_𝑦 \cdot \frac{𝑌}{𝑍}+𝑐_𝑦
\end{equation}


#### Camera Intrisic Parameter

These are the parameters that characterize the optical, geometric, and digital characteristics of the camera:

(1) the perspective projection (focal length $f_x$ and $f_y$ ).
(2) the transformation (center $c_x$ and $c_y$) between image plane coordinates and pixel coordinates.
(3) the geometric distortion introduced by the optics. (We omit it in this lecture for simplicity)

They are necessary to link the pixel coordinates of an image point with the corresponding coordinates in the camera coordinate system.


#### Camera Extrinsic Parameter

Camera extrinsic parameters define the location and orientation of the camera coordinate system with respect to the world coordinate system.

(1) The translation vector $T$ between the relative positions of the two origins of the coordinate system.
(2) The rotation matrix $R$ that brings the corresponding axes of the two coordinate systems into alignment (i.e., onto each other)


#### 3D Pose Estimation via Triangulation

![](https://i.imgur.com/QBAlU0f.png)


![](https://i.imgur.com/Gv4zYU5.png)


---

## 4. Quiz


### 1. How to implement the hourglass module?

Questions: Please complete the missing codes (marked with #TODO). You do not need to modify other codes.

```python
class HourglassModule(nn.Module):
    """Hourglass Module for HourglassNet backbone.
    Generate module recursively and use BasicBlock as the base unit.
    Args:
        depth (int): Depth of current HourglassModule.
        stage_channels (list[int]): Feature channels of sub-modules in current
            and follow-up HourglassModule.
        stage_blocks (list[int]): Number of sub-modules stacked in current and
            follow-up HourglassModule.
        norm_cfg (dict): Dictionary to construct and config norm layer.
    """

    def __init__(self,
                 depth,
                 stage_channels,
                 stage_blocks,
                 norm_cfg=dict(type='BN', requires_grad=True)):
        # Protect mutable default arguments
        norm_cfg = copy.deepcopy(norm_cfg)
        super().__init__()

        self.depth = depth

        cur_block = stage_blocks[0]
        next_block = stage_blocks[1]

        cur_channel = stage_channels[0]
        next_channel = stage_channels[1]
        
        #TODO: Please complete this function
        

    def forward(self, x):
        """Model forward function."""
        #TODO: Please complete this function
        
```


### 2. How to compute Part Affinity Fields (PAFs) in OpenPose? 

Questions: Please complete the missing codes in `_accumulate_paf_map_` (marked with #TODO). You do not need to modify other codes.


```python
class PAFGenerator:
    """Generate part affinity fields.
    Args:
        output_size (int): Size of feature map.
        limb_width (int): Limb width of part affinity fields.
        skeleton (list[list]): connections of joints.
    """

    def __init__(self, output_size, limb_width, skeleton):
        self.output_size = output_size
        self.limb_width = limb_width
        self.skeleton = skeleton

    def _accumulate_paf_map_(self, pafs, src, dst, count):
        """Accumulate part affinity fields between two given joints.
        Args:
            pafs (np.ndarray[2xHxW]): paf maps (2 dimensions:x axis and
                y axis) for a certain limb connection. This argument will
                be modified inplace.
            src (np.ndarray[2,]): coordinates of the source joint.
            dst (np.ndarray[2,]): coordinates of the destination joint.
            count (np.ndarray[HxW]): count map that preserves the number
                of non-zero vectors at each point. This argument will be
                modified inplace.
        """
        #TODO: Please complete this function.

        return pafs, count

    def __call__(self, joints):
        """Generate the target part affinity fields."""
        pafs = np.zeros(
            (len(self.skeleton) * 2, self.output_size, self.output_size),
            dtype=np.float32)

        for idx, sk in enumerate(self.skeleton):
            count = np.zeros((self.output_size, self.output_size),
                             dtype=np.float32)

            for p in joints:
                src = p[sk[0]]
                dst = p[sk[1]]
                if src[2] > 0 and dst[2] > 0:
                    self._accumulate_paf_map_(pafs[2 * idx:2 * idx + 2],
                                              src[:2], dst[:2], count)

            pafs[2 * idx:2 * idx + 2] /= np.maximum(count, 1)

        return pafs
```



### 3. How to compute 3D poses via Triangulation?

Question:
```python
# camera 1
R1 = [[1, 0, 0],
      [0, 1, 0],
      [0, 0, 1]]
T1 = [[0], [0], [0]]
K1 = [[1, 0, 1],
     [0, 2, 2],
     [0, 0, 1]]
# camera 2
R2 = [[0, -1, 0],
      [-1,  0, 0],
      [0,  0, -1]]
T2 = [[2], [2], [2]]
K2 = [[2, 0, 0], 
     [0, 1, 2],
     [0, 0, 1]]

# 2D Keypoint location
P1 = [[2], [4]] # camera-view 1
P2 = [[2], [3]] # camera-view 2
```

(1) We can solve the equation, and obtain $𝑃^{𝑤𝑜𝑟𝑙𝑑} = c*[[1],  [1],  [1]]$.
```python
P1_h = [[2], [4], [1]]
A1 = P1_h x (K1 @ R1)
   = [[0, -2, 2],
      [1, 0, -1],
      [-4, 4, 0]]
b1 = [[0], [0], [0]]    

    
P2_h = [[2], [3], [1]]
A2 = P2_h x (K2 @ R2)
   = [[1, 0, -1],
      [0, -2, 2],
      [-2, 6, -4]]
b2 = [[0], [0], [0]] 

p_world = c * [[1], [1], [1]], where c in (0, 2) is a constant
```


(2) Why is there no unique solution?



## References

<text id="review1"> [1]: https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5
<text id="review2"> [2]: https://towardsdatascience.com/cvpr-2017-openpose-realtime-multi-person-2d-pose-estimation-using-part-affinity-fields-f2ce18d720e8
