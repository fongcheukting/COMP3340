{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Preliminary\n",
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)\n",
    "\n",
    "# modify the code if you are using multiply GPUs\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model architecture\n",
    "This section is used to define MobileNetv2 model and there is a fuse_model function for fusing layers (used for torch quantization).\n",
    "Several notable modifications to enable torch quantization (it is fine if you do not use torch quantization):\n",
    "- Replacing addition with quantized. FloatFunctional\n",
    "- Insert QuantStub and DeQuantStub ar the beginning and the end of the network\n",
    "- Replace ReLU6 with ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(out_planes, momentum=0.1),\n",
    "            # Replace with ReLU\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, torch_quan=False):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.torch_quan = torch_quan\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup, momentum=0.1),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        # Replace torch.add with floatfunctional\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            if self.torch_quan:\n",
    "                return self.skip_add.add(x, self.conv(x))\n",
    "            else:\n",
    "                return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8, torch_quan=False):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.torch_quan = torch_quan\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, torch_quan=self.torch_quan))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "        \"\"\"there are static quantize\"\"\"\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.torch_quan:\n",
    "            x = self.quant(x)\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3]) # maybe this function as [average pool + flatten]\n",
    "        # x = nn.functional.adaptive_avg_pool2d(x, 1).reshape(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        if self.torch_quan:\n",
    "            x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n",
    "    # This operation does not change the numerics\n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == ConvBNReLU:\n",
    "                torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
    "            if type(m) == InvertedResidual:\n",
    "                for idx in range(len(m.conv)):\n",
    "                    if type(m.conv[idx]) == nn.Conv2d:\n",
    "                        torch.quantization.fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Helper functions\n",
    "Define several helper functions to help with model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "    \n",
    "def evaluate(model, criterion, data_loader, device=torch.device('cpu')):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "\n",
    "def load_model(model_file, torch_quan=True):\n",
    "    model = MobileNetV2(torch_quan=torch_quan)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define dataset and data loaders\n",
    "The dataset used for training and testing is [ImageNet 1000(mini)](https://www.kaggle.com/ifigotin/imagenetmini-1000). With the data downloaded, you can use prepare_data_loaders to read in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loaders(data_path):\n",
    "    traindir = os.path.join(data_path, 'train')\n",
    "    valdir = os.path.join(data_path, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        traindir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    print(\"dataset_train : %d\" % (len(dataset)))\n",
    "\n",
    "    dataset_test = torchvision.datasets.ImageFolder(\n",
    "        valdir,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    print(\"dataset_test : %d\" % (len(dataset_test)))\n",
    "    num_test = len(dataset_test)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test, num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train : 34745\n",
      "dataset_test : 3923\n"
     ]
    }
   ],
   "source": [
    "data_path = 'imagenet-mini'\n",
    "train_batch_size = 30\n",
    "eval_batch_size = 30\n",
    "\n",
    "data_loader, data_loader_test, num_test = prepare_data_loaders(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load pretrained model and fuse model for torch quantization.\n",
    "The [pretrained model](https://download.pytorch.org/models/mobilenet_v2-b0353104.pth) is from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = 'model/'\n",
    "float_model_file = 'mobilenet_v2-b0353104.pth'\n",
    "scripted_float_model_file = 'mobilenet_quantization_scripted.pth'\n",
    "scripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inverted Residual Block: Before fusion \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      " Inverted Residual Block: After fusion\n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNReLU(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (2): Identity()\n",
      ")\n",
      "Size of baseline model\n",
      "Size (MB): 13.999657\n",
      "...................................................................................................................................Evaluation accuracy on 3923 images, 71.63\n"
     ]
    }
   ],
   "source": [
    "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "# Next, we’ll “fuse modules”; this can both make the model faster by saving on memory access\n",
    "# while also improving numerical accuracy.\n",
    "# While this can be used with any model, this is especially common with quantized models.\n",
    "print('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\n",
    "float_model.eval()\n",
    "\n",
    "# Fuses modules\n",
    "float_model.fuse_model()\n",
    "\n",
    "# Note fusion of Conv+BN+Relu and Conv+Relu\n",
    "print('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "top1, top5 = evaluate(float_model, criterion, data_loader_test, torch.device(device))\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_test, top1.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)\n",
    "\n",
    "del float_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use torch quantization to do the post-training static quantization\n",
    "In post-training quantization, model just be converted from float to int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n",
      "Post Training Quantization Prepare: Inserting Observers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xupeng/miniconda3/envs/bell/lib/python3.8/site-packages/torch/quantization/observer.py:121: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................Post Training Quantization: Calibration done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xupeng/miniconda3/envs/bell/lib/python3.8/site-packages/torch/quantization/observer.py:955: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Convert done\n",
      "Size of model after quantization\n",
      "Size (MB): 3.950561\n",
      "...................................................................................................................................Evaluation accuracy on 3923 images, 67.17\n"
     ]
    }
   ],
   "source": [
    "myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "myModel.eval()\n",
    "\n",
    "# Fuse Conv, bn and relu\n",
    "myModel.fuse_model()\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "myModel.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "print(myModel.qconfig)\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# Calibrate with the training set\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "evaluate(myModel, criterion, data_loader_test, torch.device(device))\n",
    "print('Post Training Quantization: Calibration done')\n",
    "\n",
    "# Convert to quantized model\n",
    "myModel.to('cpu')\n",
    "torch.quantization.convert(myModel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)\n",
    "\n",
    "top1, top5 = evaluate(myModel, criterion, data_loader_test)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_test, top1.avg))\n",
    "torch.jit.save(torch.jit.script(myModel), saved_model_dir + scripted_quantized_model_file)\n",
    "\n",
    "del myModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use torch quantization to do the Quantization-aware training\n",
    "Quantization-aware training (QAT) is the quantization method that typically results in the highest accuracy. With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method will usually yield higher accuracy than either dynamic quantization or post-training static quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n",
    "    model.train() # must be in train mode\n",
    "    model.to(device)\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    avgloss = AverageMeter('Loss', '1.5f')\n",
    "    cnt = 0\n",
    "\n",
    "    for image, target in data_loader:\n",
    "        start_time = time.time()\n",
    "        print('.', end = '')\n",
    "        cnt += 1\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "        avgloss.update(loss, image.size(0))\n",
    "        if cnt >= ntrain_batches:\n",
    "            print('Loss', avgloss.avg)\n",
    "\n",
    "            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "                  .format(top1=top1, top5=top5))\n",
    "            return\n",
    "\n",
    "    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................Loss tensor(1.7438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 60.167 Acc@5 81.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xupeng/miniconda3/envs/bell/lib/python3.8/site-packages/torch/quantization/observer.py:243: UserWarning: must run observer before calling calculate_qparams.                                        Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................Epoch 0 :Evaluation accuracy on 3923 images, 67.30\n",
      "....................Loss tensor(1.5380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 65.000 Acc@5 84.500\n",
      "...................................................................................................................................Epoch 1 :Evaluation accuracy on 3923 images, 66.81\n",
      "....................Loss tensor(1.7409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 61.333 Acc@5 81.167\n",
      "...................................................................................................................................Epoch 2 :Evaluation accuracy on 3923 images, 67.22\n",
      "....................Loss tensor(1.7945, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 57.500 Acc@5 81.333\n",
      "...................................................................................................................................Epoch 3 :Evaluation accuracy on 3923 images, 67.78\n",
      "....................Loss tensor(1.6470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 63.667 Acc@5 82.500\n",
      "...................................................................................................................................Epoch 4 :Evaluation accuracy on 3923 images, 66.71\n",
      "....................Loss tensor(1.6096, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 62.167 Acc@5 83.167\n",
      "...................................................................................................................................Epoch 5 :Evaluation accuracy on 3923 images, 67.32\n",
      "....................Loss tensor(1.3927, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 67.833 Acc@5 86.167\n",
      "...................................................................................................................................Epoch 6 :Evaluation accuracy on 3923 images, 67.68\n",
      "....................Loss tensor(1.5978, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 63.167 Acc@5 83.000\n",
      "...................................................................................................................................Epoch 7 :Evaluation accuracy on 3923 images, 67.52\n"
     ]
    }
   ],
   "source": [
    "qat_model = load_model(saved_model_dir + float_model_file)\n",
    "qat_model.fuse_model()\n",
    "\n",
    "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
    "num_train_batches = 20\n",
    "num_epochs = 8\n",
    "\n",
    "# Train and check accuracy after each epoch\n",
    "for nepoch in range(num_epochs):\n",
    "    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device(device), num_train_batches)\n",
    "    if nepoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        qat_model.apply(torch.quantization.disable_observer)\n",
    "    if nepoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    # Check the accuracy after each epoch\n",
    "    qat_model.to('cpu')\n",
    "    quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n",
    "    quantized_model.eval()\n",
    "    top1, top5 = evaluate(quantized_model, criterion, data_loader_test)\n",
    "    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_test, top1.avg))\n",
    "    \n",
    "del qat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Speedup from torch quantization\n",
    "Test whether quantized model actually perform inference faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 289 ms\n",
      "Elapsed time:  26 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39.52912259101868"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_benchmark(model_file, img_loader, device=torch.device('cpu')):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    num_batches = 50\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        images, target = images.to(device), target.to(device)\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "\n",
    "run_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n",
    "run_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Quantize the model with self-defined methods\n",
    "There are many methods for QAT and torch quantization is mainly referred to this [article](https://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf). Therefore, you also define the other quantization algorithm you like to perform QAT.\n",
    "\n",
    "This section use [LSQ](https://deepai.org/publication/learned-step-size-quantization) as the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSQ_Conv2D(nn.Conv2d):\n",
    "    def __init__(self, m: nn.Conv2d, bits, weight_quantizer, act_quantizer):\n",
    "        super(LSQ_Conv2D, self).__init__(\n",
    "            in_channels=m.in_channels,\n",
    "            out_channels=m.out_channels,\n",
    "            kernel_size=m.kernel_size,\n",
    "            stride=m.stride,\n",
    "            padding=m.padding,\n",
    "            dilation=m.dilation,\n",
    "            groups=m.groups,\n",
    "            bias=True if m.bias is not None else False,\n",
    "            padding_mode=m.padding_mode\n",
    "        )\n",
    "\n",
    "        self.weight = nn.Parameter(m.weight.detach())\n",
    "        self.bits = bits\n",
    "\n",
    "        self.weight_quantizer = weight_quantizer\n",
    "        self.weight_quantizer.init_step_size(m.weight)\n",
    "\n",
    "        self.act_quantizer = act_quantizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        quantized_weight = self.weight_quantizer(self.weight)\n",
    "\n",
    "        quantized_act = self.act_quantizer(x)\n",
    "\n",
    "        # quantized_act = x\n",
    "\n",
    "        return F.conv2d(quantized_act, quantized_weight, self.bias, \n",
    "                        self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "class LSQ_Linear(nn.Linear):\n",
    "    def __init__(self, m: nn.Conv2d, bits, weight_quantizer, act_quantizer):\n",
    "        super(LSQ_Linear, self).__init__(\n",
    "            in_features=m.in_features,\n",
    "            out_features=m.out_features,\n",
    "            bias=True if m.bias is not None else False)\n",
    "\n",
    "        self.weight = nn.Parameter(m.weight.detach())\n",
    "        self.bits = bits\n",
    "        self.weight_quantizer = weight_quantizer\n",
    "\n",
    "        self.weight_quantizer.init_step_size(m.weight)\n",
    "        self.act_quantizer = act_quantizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        quantized_weight = self.weight_quantizer(self.weight)\n",
    "\n",
    "        quantized_act = self.act_quantizer(x)\n",
    "\n",
    "        # quantized_act = x\n",
    "\n",
    "        return F.linear(quantized_act, quantized_weight, self.bias)\n",
    "\n",
    "\n",
    "QuanModuleMapping = {\n",
    "    nn.Conv2d: LSQ_Conv2D,\n",
    "    nn.Linear: LSQ_Linear\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSQ_Quantizer(nn.Module):\n",
    "    def __init__(self, bits, is_activation=False):\n",
    "        super(LSQ_Quantizer, self).__init__()\n",
    "\n",
    "        self.bits = bits\n",
    "\n",
    "        if(is_activation):\n",
    "            self.Qn = 0\n",
    "            self.Qp = 2 ** bits - 1\n",
    "        else:\n",
    "            self.Qn = -2**(bits - 1)\n",
    "            self.Qp = 2 ** (bits - 1) - 1\n",
    "\n",
    "        self.s = nn.Parameter(torch.Tensor([1.0]))\n",
    "\n",
    "    def init_step_size(self, x):\n",
    "        self.s = nn.Parameter(\n",
    "            x.detach().abs().mean() * 2 / (self.Qp) ** 0.5)\n",
    "\n",
    "    def grad_scale(self, x, scale):\n",
    "        y_out = x\n",
    "        y_grad = x * scale\n",
    "\n",
    "        y = y_out.detach() - y_grad.detach() + y_grad\n",
    "\n",
    "        return y\n",
    "\n",
    "    def round_pass(self, x):\n",
    "        y_out = x.round()\n",
    "        y_grad = x\n",
    "        y = y_out.detach() - y_grad.detach() + y_grad\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale_factor = 1 / (x.numel() * self.Qp) ** 0.5\n",
    "\n",
    "        scale = self.grad_scale(self.s, scale_factor)\n",
    "        x = x / scale\n",
    "        x = x.clamp(self.Qn, self.Qp)\n",
    "\n",
    "        x_bar = self.round_pass(x)\n",
    "\n",
    "        x_hat = x_bar * scale\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model, bits=8):\n",
    "    def helper(child: nn.Module):\n",
    "        for n, c in child.named_children():\n",
    "            if type(c) in QuanModuleMapping.keys():\n",
    "                for full_name, m in model.named_modules():\n",
    "                    if c is m:\n",
    "                        child.add_module(n, modules_to_replace.pop(full_name))\n",
    "                        break\n",
    "            else:\n",
    "                helper(c)\n",
    "    \n",
    "    modules_to_replace = dict()\n",
    "    for name, module in model.named_modules():\n",
    "        if type(module) in QuanModuleMapping.keys():\n",
    "            modules_to_replace[name] = QuanModuleMapping[type(module)](\n",
    "                module,\n",
    "                bits,\n",
    "                LSQ_Quantizer(bits, False),\n",
    "                LSQ_Quantizer(bits, True)\n",
    "            )\n",
    "\n",
    "    helper(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Loss tensor(7.1064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 0.187 Acc@5 0.787\n",
      "...................................................................................................................................Epoch 0 :Evaluation accuracy on 3923 images, 0.18\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Loss tensor(7.0213, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 0.320 Acc@5 1.113\n",
      "...................................................................................................................................Epoch 1 :Evaluation accuracy on 3923 images, 0.23\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Loss tensor(6.9663, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 0.360 Acc@5 1.447\n",
      "...................................................................................................................................Epoch 2 :Evaluation accuracy on 3923 images, 0.25\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Loss tensor(6.9428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 0.447 Acc@5 1.500\n",
      "...................................................................................................................................Epoch 3 :Evaluation accuracy on 3923 images, 0.23\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Loss tensor(6.8697, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 0.400 Acc@5 1.940\n",
      "...................................................................................................................................Epoch 4 :Evaluation accuracy on 3923 images, 0.15\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Loss tensor(6.8380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 0.600 Acc@5 1.853\n",
      "...................................................................................................................................Epoch 5 :Evaluation accuracy on 3923 images, 0.15\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Loss tensor(6.7938, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 0.460 Acc@5 2.013\n",
      "...................................................................................................................................Epoch 6 :Evaluation accuracy on 3923 images, 0.33\n",
      "....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Loss tensor(6.7787, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 0.493 Acc@5 2.033\n",
      "...................................................................................................................................Epoch 7 :Evaluation accuracy on 3923 images, 0.41\n"
     ]
    }
   ],
   "source": [
    "lsq_model = load_model(saved_model_dir + float_model_file, False)\n",
    "lsq_model = quantize_model(lsq_model)\n",
    "\n",
    "optimizer = torch.optim.SGD(lsq_model.parameters(), lr = 0.003, momentum=0.9)\n",
    "num_train_batches = 500\n",
    "num_epochs = 8\n",
    "\n",
    "# Train and check accuracy after each epoch\n",
    "for nepoch in range(num_epochs):\n",
    "    train_one_epoch(lsq_model, criterion, optimizer, data_loader, torch.device(device), num_train_batches)\n",
    "\n",
    "    # Check the accuracy after each epoch\n",
    "    top1, top5 = evaluate(lsq_model, criterion, data_loader_test, torch.device(device))\n",
    "    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_test, top1.avg))\n",
    "    \n",
    "del lsq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Project Description\n",
    "In this project, you need to quantize a Yolo network to do the objection detection task.\n",
    "- The DL framework should be Pytorch\n",
    "- Requires dataset is coco2017\n",
    "- The bit number of quantized model weight is up to 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "- [MobileNetV2 implementation and helper functions](https://github.com/pytorch/vision/blob/main/torchvision/models/mobilenetv2.py)\n",
    "- [The usage of torch quantization](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html)\n",
    "- [Learnable Step Size Quantization(LSQ) implementation](https://github.com/Kelvinyu1117/LSQ-implementation)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7101466c9df93df25745258736fd1570769bbd7a8dcce1fe8dcc313200c13bc"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
