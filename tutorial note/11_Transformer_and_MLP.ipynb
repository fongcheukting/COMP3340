{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11_Transformer_and_MLP_chongjiange_shoufa.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_eR_dYhTUswO"},"source":["# Tutorial 9: Classification with Transformers and MLPs\n","\n","\n","In this tutorial, you will learn\n","\n","- How to build a Transformer or MLP model\n","- How to implement a specific data augmentation\n","- How to use a Transformer or MLP model for classification\n","\n","\n","## Step 1: Install\n","\n","The requirements for the classification are as follow:\n","```\n","Ubuntu\n","Nvidia RTX 2080\n","\n","cuda=='11.0'\n","python=='3.7'\n","pytorch=='1.7.1'\n","torchvision=='0.8.2'\n","mmcv=='1.3.14'\n","mmcls=='0.15.0'\n","```\n","\n","\n","The following shows how to install mmclassification from scratch. We Create a conda virtual environment and activate it.\n","```shell\n","(base) ➜  mmclassification git:(master) git checkout -b dev-tutorial v0.16.0 \n","Switched to a new branch 'dev-tutorial'\n","\n","# create an environment named 'comp3340', with python=3.8\n","conda create -n comp3340 python=3.8\n","\n","conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge\n","\n","pip install mmcv-full==1.3.8\n","\n","cd /work/path/to/mmclassification\n","pip install -e .\n","\n","```\n","\n","### Check Installation\n","```\n","python -c \"import mmcls; print(mmcls.__version__)\"\n","\n","# Install successfully if get:\n","0.16.0\n","```"]},{"cell_type":"code","metadata":{"id":"D6tdb8aAb7BY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"odKN3yAr23Na"},"source":["## Step 2. Data Preparing\n","\n","We use [Category Flower Dataset](https://www.robots.ox.ac.uk/~vgg/data/flowers/17/index.html) for this tutorial.\n","\n","\n","This dataset contains 17 category flower dataset with 80 images for each class. The flowers chosen are some common flowers in the UK. The images have large scale, pose and light variations and there are also classes with large varations of images within the class and close similarity to other classes. The categories can be seen in the figure below. The dataset is randomly split into 3 different training, validation and test sets. A subset of the images have been groundtruth labelled for segmentation.\n","\n","<!-- ![Image example](https://www.robots.ox.ac.uk/~vgg/data/flowers/17/categories.jpg) -->\n","\n","\n","### Download and Split Data\n","\n","Let `$DATA_ROOT` denote the path of dataset. E.g., `/home/chenshoufa/share_data/comp3340`.\n","```bash\n","cd $DATA_ROOT\n","# download\n","wget https://www.robots.ox.ac.uk/~vgg/data/flowers/17/17flowers.tgz\n","tar zxvf 17flowers.tgz\n","\n","# rename folder\n","mv jpg flowers\n","\n","mkdir data\n","mv 17flowers data/flowers\n","\n","# split\n","python split.py\n","\n","# meta file\n","mkdir meta\n","python generate_meta.py\n","\n","```\n","\n","The foloder structure:\n","```\n","flowers\n","     |--train\n","          |--class_0\n","                |--image_xxxx.jpg\n","                |--image_xxxx.jpg\n","          |--class_1\n","                |--image_xxxx.jpg\n","     |--val\n","          |--class_0\n","                |--image_xxxx.jpg\n","                |--image_xxxx.jpg\n","          |--class_1\n","                |--image_xxxx.jpg  \n","    |--meta\n","        |--train.txt\n","        |--val.txt\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"rdb0RXMTz0WW"},"source":["### Data Loader\n","\n","\n","\n","In ```mmclassification/mmcls```, we follow the file 'imagenet.py', which writes the dataset class of Imagenet, to write a dataset class file 'flowers.py'. Since we follow the data directory of Imagenet above, we just need to copy 'imagenet.py' and re-write the **CLASSES** here.  \n","```python\n","@DATASETS.register_module()\n","class Flowers(BaseDataset):\n","\n","    IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif')\n","    CLASSES = [\n","        'daffodil', 'snowdrop', 'lilyValley', 'bluebell', 'crocys', 'iris', 'tigerlily', 'tulip', 'fritillary', 'sunflower', 'daisy', 'colts foot', 'dandelion', 'cowslip', 'buttercup', 'wind flower', 'pansy'\n","    ]\n","\n","    def load_annotations(self):\n","        if self.ann_file is None:\n","            folder_to_idx = find_folders(self.data_prefix)\n","            samples = get_samples(\n","                self.data_prefix,\n","                folder_to_idx,\n","                extensions=self.IMG_EXTENSIONS)\n","            if len(samples) == 0:\n","                raise (RuntimeError('Found 0 files in subfolders of: '\n","                                    f'{self.data_prefix}. '\n","                                    'Supported extensions are: '\n","                                    f'{\",\".join(self.IMG_EXTENSIONS)}'))\n","\n","            self.folder_to_idx = folder_to_idx\n","        elif isinstance(self.ann_file, str):\n","            with open(self.ann_file) as f:\n","                samples = [x.strip().rsplit(' ', 1) for x in f.readlines()]\n","        else:\n","            raise TypeError('ann_file must be a str or None')\n","        self.samples = samples\n","\n","        data_infos = []\n","        for filename, gt_label in self.samples:\n","            info = {'img_prefix': self.data_prefix}\n","            info['img_info'] = {'filename': filename}\n","            info['gt_label'] = np.array(gt_label, dtype=np.int64)\n","            data_infos.append(info)\n","        return data_infos\n","```\n","\n","We also need to import `Flower` class in the `mmcls/data/sets/__init__.py`:\n","\n","```diff\n"," from .voc import VOC\n","+from .flowers import Flowers\n"," \n"," __all__ = [\n","     'BaseDataset', 'ImageNet', 'CIFAR10', 'CIFAR100', 'MNIST', 'FashionMNIST',\n","     'VOC', 'MultiLabelDataset', 'build_dataloader', 'build_dataset', 'Compose',\n","     'DistributedSampler', 'ConcatDataset', 'RepeatDataset',\n","-    'ClassBalancedDataset', 'DATASETS', 'PIPELINES'\n","+    'ClassBalancedDataset', 'DATASETS', 'PIPELINES', 'Flowers',\n"," ]\n","\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"UPh47PS7z82b"},"source":["## Configuration\n","\n","\n","Now we are at `mmclassification/configs/_base_` directory\n","\n","### set data config\n","a. In `_base_/datasets`, we add `flowers_bs32.py` to describle the basic config about data.\n","    Note that we need to claim the data meta path in it like that\n","```shell\n","    data = dict(\n","    samples_per_gpu=32,\n","    workers_per_gpu=1,\n","    train=dict(\n","        type=dataset_type,\n","        data_prefix='data/flowers/train',\n","        ann_file='data/flowers/meta/train.txt',\n","        pipeline=train_pipeline),\n","    val=dict(\n","        type=dataset_type,\n","        data_prefix='data/flowers/val',\n","        ann_file='data/flowers/meta/val.txt',\n","        pipeline=test_pipeline),\n","    test=dict(\n","        # replace `data/val` with `data/test` for standard test\n","        type=dataset_type,\n","        data_prefix='data/flowers/val',\n","        ann_file='data/flowers/meta/val.txt',\n","        pipeline=test_pipeline))\n","    evaluation = dict(interval=1, metric='accuracy')\n","```\n","\n","### Building Vision Transformer\n","b. In `_base_/models`, we add `vit_base_flowers.py` to describle the basic config about model.\n","\n","```shell\n","    type='ImageClassifier',\n","    backbone=dict(\n","        return_tuple=False,\n","        type='VisionTransformer',\n","        num_layers=12,\n","        embed_dim=768,\n","        num_heads=12,\n","        img_size=224,\n","        patch_size=16,\n","        in_channels=3,\n","        feedforward_channels=3072,\n","        drop_rate=0.1,\n","        attn_drop_rate=0.),\n","    neck=None,\n","    head=dict(\n","        type='VisionTransformerClsHead',\n","        num_classes=17,  # modify\n","        in_channels=768,\n","        hidden_dim=3072,\n","        loss=dict(type='LabelSmoothLoss', label_smooth_val=0.1),\n","        topk=(1, 5),\n","    ),\n","    train_cfg=dict(\n","        augments=dict(type='BatchMixup', alpha=0.2, num_classes=17,  # modify\n","                      prob=1.))\n",")\n","```\n","Note we use the `ViT-Base` model, and set `num_classes` as 17 since the number of flower categories is 17.\n","\n","### Set training config\n","c. In `_base_/schedules`, we add `flowers_bs32.py`\n","```shell\n","paramwise_cfg = dict(\n","    norm_decay_mult=0.0,\n","    bias_decay_mult=0.0,\n","    custom_keys={\n","        '.absolute_pos_embed': dict(decay_mult=0.0),\n","        '.relative_position_bias_table': dict(decay_mult=0.0)\n","    })\n","\n","# for batch in each gpu is 128, 8 gpu\n","# lr = 5e-4 * 128 * 8 / 512 = 0.001\n","optimizer = dict(\n","    type='AdamW',\n","    # lr=5e-4 * 128 * 8 / 512,\n","    lr=5e-4 * 16 / 512,\n","    weight_decay=0.05,\n","    eps=1e-8,\n","    betas=(0.9, 0.999),\n","    paramwise_cfg=paramwise_cfg)\n","optimizer_config = dict(grad_clip=dict(max_norm=5.0))\n","\n","# learning policy\n","lr_config = dict(\n","    policy='CosineAnnealing',\n","    by_epoch=False,\n","    min_lr_ratio=1e-2,\n","    warmup='linear',\n","    warmup_ratio=1e-3,\n","    warmup_iters=20 * 1252,\n","    warmup_by_epoch=False)\n","\n","runner = dict(type='EpochBasedRunner', max_epochs=300)\n","\n","```\n","\n","### AutoAugmentation\n","\n","```python\n","\n","policies = [\n","    [\n","        dict(type='Posterize', bits=4, prob=0.4),\n","        dict(type='Rotate', angle=30., prob=0.6)\n","    ],\n","    [\n","        dict(type='Solarize', thr=256 / 9 * 4, prob=0.6),\n","        dict(type='AutoContrast', prob=0.5)\n","    ],\n","...\n","    [\n","        dict(type='ColorTransform', magnitude=0.4, prob=0.6),\n","        dict(type='Contrast', magnitude=0.8, prob=1.)\n","    ],\n","    [dict(type='Equalize', prob=0.8),\n","     dict(type='Equalize', prob=0.6)],\n","]\n","```\n","\n","### set saving config\n","d. In the ```_base_/default_runtime.py```, we set the config about checkpoint saving and log file.\n","```\n","    # checkpoint saving\n","    checkpoint_config = dict(interval=1)\n","    # yapf:disable\n","    log_config = dict(\n","        interval=100,\n","        hooks=[\n","            dict(type='TextLoggerHook'),\n","            # dict(type='TensorboardLoggerHook')\n","        ])\n","    # yapf:enable\n","\n","    dist_params = dict(backend='nccl')\n","    log_level = 'INFO'\n","    load_from = None\n","    resume_from = None\n","    workflow = [('train', 1)]\n","```\n","\n","And then, we new a config file in ```mmclassification/configs/resnet/resnet18_flowers_bs128.py``` \n","we add these config .py files into this file as\n","```\n","_base_ = [\n","    '../_base_/models/vit_base_flowers.py',\n","    '../_base_/datasets/flowers_bs32.py',\n","    '../_base_/schedules/flowers_bs32.py',\n","    '../_base_/default_runtime.py'\n","]\n","```"]},{"cell_type":"markdown","metadata":{"id":"NNuqOFY90Nil"},"source":["## Train\n","\n","In the directory of 'mmclassification', we run\n","```shell\n","python tools/train.py configs/vision_transformer/vit_base_patch16_224_flowers.py\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MAnsTVhMk394"},"source":["Logs:\n","\n","```\n","2021-10-11 17:16:01,923 - mmcls - INFO - Epoch [2][10/75]       lr: 6.799e-08, eta: 1:06:39, time: 0.355, data_time: 0.236, memory: 4064, loss: 2.8331, grad_norm: 7.4339\n","2021-10-11 17:16:03,150 - mmcls - INFO - Epoch [2][20/75]       lr: 7.422e-08, eta: 1:04:26, time: 0.123, data_time: 0.003, memory: 4064, loss: 2.8331, grad_norm: 7.2677\n","2021-10-11 17:16:04,365 - mmcls - INFO - Epoch [2][30/75]       lr: 8.045e-08, eta: 1:02:35, time: 0.121, data_time: 0.003, memory: 4064, loss: 2.8331, grad_norm: 7.4220\n","2021-10-11 17:16:05,570 - mmcls - INFO - Epoch [2][40/75]       lr: 8.668e-08, eta: 1:01:02, time: 0.121, data_time: 0.003, memory: 4064, loss: 2.8331, grad_norm: 7.1345 \n","2021-10-11 17:16:06,838 - mmcls - INFO - Epoch [2][50/75]       lr: 9.292e-08, eta: 0:59:54, time: 0.127, data_time: 0.004, memory: 4064, loss: 2.8330, grad_norm: 7.3180\n","2021-10-11 17:16:08,087 - mmcls - INFO - Epoch [2][60/75]       lr: 9.915e-08, eta: 0:58:53, time: 0.125, data_time: 0.004, memory: 4064, loss: 2.8330, grad_norm: 7.2354\n","2021-10-11 17:16:09,308 - mmcls - INFO - Epoch [2][70/75]       lr: 1.054e-07, eta: 0:57:56, time: 0.122, data_time: 0.004, memory: 4064, loss: 2.8330, grad_norm: 7.3601\n","2021-10-11 17:16:09,999 - mmcls - INFO - Saving checkpoint at 2 epochs\n","[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 170/170, 181.1 task/s, elapsed: 1s, ETA:     0s2021-10-11 17:16:12,832 - mmcls - INFO - Epoch(val) [2][11] accuracy_top-1: 20.5882, accuracy_top-5: 64.70\n","59\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"IaoYzqDxruVP"},"source":["## Evaluation\n","\n","```bash\n","python tools/test.py configs/vision_transformer/vit_base_patch16_224_flowers.py path/to/checkpoint.pth\n","```"]},{"cell_type":"markdown","metadata":{"id":"CRsYoq9Ib9JO"},"source":["## Group Project Description\n","\n","* In this project, you need to implement at least one MLP-based model (e.g., MLP-Mixer, ResMLP, gMLP) based on MMClassification codebase; \n","* You need to train your model on the  Category Flower Dataset, which contains 17 category flower dataset with 80 images for each class. After training, you need test your model on the validation set. \n","* Bonus: Implement more than one MLP-based models and discuss their similarities and differences."]},{"cell_type":"markdown","source":["## References\n","\n","[1] Tolstikhin, Ilya, et al. \"Mlp-mixer: An all-mlp architecture for vision.\" NeurIPS, 2021.\n","\n","[2] OpenMMLab's Image Classification Toolbox and Benchmark. https://github.com/open-mmlab/mmclassification\n"],"metadata":{"id":"G5hWnZW8rAg5"}}]}